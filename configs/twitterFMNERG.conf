

train_path = data/datasets/twitterFMNERG/train.txt
valid_path = data/datasets/twitterFMNERG/dev.txt
types_path = data/datasets/twitterFMNERG/twitter10k_types.json
# annotation path
xml_path = data/datasets/images_annotation
#detection model path
detection_path = data/datasets/Vinvl_detection_path
save_path = data/checkpoints/twitterFMNERG
# raw image path
image_path = data/datasets/raw_images
# textual model
model_path = bert-base-cased
tokenizer_path = bert-base-cased
# vision model
vit_name = openai/clip-vit-base-patch32
train_batch_size = 8
epochs = 50
lr = 2e-05
lr_warmup = 0.05
weight_decay = 0.01
max_grad_norm = 1.0
# set preidction setting
match_solver = hungarian
# query Number
entity_queries_num = 60
type_loss = celoss
match_warmup_epoch = 0
nil_weight = -1.0
match_boundary_weight = 2.0
match_class_weight = 2.0
loss_boundary_weight = 2.0
loss_class_weight = 2.0
use_aux_loss = True
deeply_weight = same
repeat_gt_entities = 45
split_epoch = 5
local_rank = -1
world_size = -1
sampling_processes = 4
label = twitter10k_train
log_path = data/checkpoints/twitterFMNERG
device_id = 2
model_type = MQSPN
cpu = False
eval_batch_size = 8
prop_drop = 0.5
freeze_transformer = False
no_overlapping = True
no_partial_overlapping = True
no_duplicate = True
cls_threshold = 0.0
boundary_threshold = 0.0
pos_size = 25
char_lstm_layers = 1
lstm_layers = 3
char_size = 50
char_lstm_drop = 0.2
use_glove = False
use_pos = False
use_char_lstm = False
# change to false if oom
use_lstm = False
pool_type = max
use_token_level_encoder = True
num_token_entity_encoderlayer = 5
candidate_regions_num = 15
mask_ent2tok = True
mask_tok2ent = False
mask_ent2ent = True
mask_entself = True
word_mask_ent2tok = True
word_mask_tok2ent = False
word_mask_ent2ent = True
word_mask_entself = True
use_entity_pos = True
last_layer_for_loss = 3
seed = 47

